# Code written by Terada Asavapakuna 1012869

# import libraries
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from itertools import combinations
from sklearn.decomposition import PCA


# preprocessing done in task 2a, use values from task 2a by reading output csv
# this assumes the files are in the directory
x_train_df = pd.read_csv("x_train_standardised.csv")
y_train_df = pd.read_csv("y_train.csv")
x_test_df = pd.read_csv("x_test_standardised.csv")
y_test_df = pd.read_csv("y_test.csv")


# ------------------ interaction term pairs ------------------

# use standardised values to create values for interaction term pairs for each country

# turn to array
x_train_array = x_train_df.values

# calculate polynomial features
poly = PolynomialFeatures(interaction_only=True)
features_stats = poly.fit_transform(x_train_array)

# from each array row ignore the first number - corresponds to 1 
# each element in a row corresponds to a feature
# first 20 elements in a row correspond to original features
# all other elements afterwards are generated
features_stats = np.delete(features_stats, 0, 1)

# ------------------ clustering labels ------------------

# make plots for k-means using x_train (world.csv but preprocessed and split into training set)
# permutation coords contain all permutation of coords used for k-means
train_combinations_coords = []

for i in range(len(x_train_df)):
    
    each_country_coords = combinations(list(x_train_df.iloc[i].values), 2) 

    for value in list(each_country_coords):
        train_combinations_coords.append(list(value))

# convert coordinate to array
x_train_cluster = np.array(train_combinations_coords)

# ----------------- to find optimal k - elbow method -----------------

Sum_of_squared_distances = []

# to find optimal k
K = range(1,15)

# try for each k and plot
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(x_train_array)
    Sum_of_squared_distances.append(km.inertia_)

elbow_plot = plt.plot(K, Sum_of_squared_distances, 'bx-')
elbow_plot = plt.xlabel('k')
elbow_plot = plt.ylabel('Sum_of_squared_distances')
elbow_plot = plt.title('Elbow Method to find Optimal k')
plt.savefig('task2b_elbow_plot.png')
#plt.show()
plt.close()

# from plot pick elbow point - > k = 4
kmeans = KMeans(n_clusters=4, random_state=0)

# train using training sets
kmeans.fit(x_train_cluster)

# x_test prediction
test_combinations_coords = []
for i in range(len(x_train_df)):
    
    each_country_coords = combinations(list(x_train_df.iloc[i].values), 2) 

    for value in list(each_country_coords):
        test_combinations_coords.append(list(value))

# convert coordinate to array
x_test_cluster = np.array(train_combinations_coords)

# find which cluster test set belongs to
y_pred_k_means = kmeans.predict(x_test_cluster)

# print to std output centers of clusters and test cluster numbers
print("Cluster center points:\n", kmeans.cluster_centers_)
cluster_no = kmeans.predict(kmeans.cluster_centers_)
print("Their corresponding cluster number:\n", cluster_no)
print("Predicting cluster numbers of testing set:\n", y_pred_k_means)

k_means = plt.scatter(x_train_cluster[:,0], x_train_cluster[:,1], s= 2, marker = 'x')
k_means = plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red')
k_means = plt.title('k-means showing cluster points')
k_means = plt.savefig('task2b_k_means.png')
#plt.show()
plt.close()

# ------------------ feature selection ------------------

# 1st feature - Domestic general government health expenditure per capita (current US$)
# original feature

def health_exp(x_dataframe):
    health_exp_list = []
    
    for i in range(len(x_dataframe)):
        
        # get value at index 5 for 1st feature
        health_exp_list.append(x_dataframe.iloc[i][5])
        
    return health_exp_list
        
x_train_health_exp = health_exp(x_train_df)
# for later testing with 5-NN classification
x_test_health_exp = health_exp(x_test_df)

# 2nd feature - Birth rate, crude (per 1,000 people) x 
# Maternal mortality ratio (modeled estimate, per 100,000 live births) 
# generated by interaction term pairs
# create interaction term pairs again - since we are accounting for negatives
# if two negatives are multipled we want to keep negative sign
# higher the feature value - more undeveloped -> higher chance of y being low

def maternal_death_rate(x_dataframe):
    
    maternal_death_rate_list = []
    
    for i in range(len(x_dataframe)):
        
        # get birth rate value - index 2
        birth_rate = x_dataframe.iloc[i][2]
        
        # get maternal mortality - index 9
        maternal_mor = x_dataframe.iloc[i][9]
    
        # account for negatives so really low (negative) values are still low, not positively high
        if (birth_rate < 0 and maternal_mor < 0):
            maternal_death_rate_val = -(birth_rate*maternal_mor)
            
        else:
            maternal_death_rate_val = birth_rate*maternal_mor
        
        maternal_death_rate_list.append(maternal_death_rate_val)
        
    return maternal_death_rate_list
    
x_train_maternal_dr = maternal_death_rate(x_train_df)
x_test_maternal_dr = maternal_death_rate(x_test_df)

# 3rd feature - People using at least basic sanitation services (% of population)
# original feature

def basic_sanitation(x_dataframe):
    
    basic_sanitation_list = []
    
    for i in range(len(x_dataframe)):
        
        # get value at index 5 for 1st feature
        basic_sanitation_list.append(x_dataframe.iloc[i][17])
        
    return basic_sanitation_list
    
x_train_sanitation = basic_sanitation(x_train_df)
x_test_sanitation = basic_sanitation(x_test_df)

# 4th feature - Access to electricity, rural (% of rural population) x
# Individuals using the Internet (% of population)
# generated by interaction term pairs
# higher the feature value - more developed -> higher chance of y being high

def electricity_internet(x_dataframe):
    
    electricity_internet_list = []
    
    for i in range(len(x_dataframe)):
        
        # get Access to electricity, rural (% of rural population) - index 0
        electricity = x_dataframe.iloc[i][0]
        
        # get Individuals using the Internet (% of population) - index 6
        internet = x_dataframe.iloc[i][6]
    
        # account for negatives so really low (negative) values are still low, not positively high
        if (electricity < 0 and internet < 0):
            electricity_internet_val = -(electricity*internet)
            
        else:
            electricity_internet_val = electricity*internet
        
        electricity_internet_list.append(electricity_internet_val)
        
    return electricity_internet_list
    
x_train_electricity_internet = electricity_internet(x_train_df)
x_test_electricity_internet = electricity_internet(x_test_df)

# create training/testing set with 4 features collected
def create_x_data_set(first_list, second_list, third_list, fourth_list):
    
    country_row = []
    full_data = []
    for i in range(len(first_list)):
        first_val = first_list[i]
        second_val = second_list[i]
        third_val = third_list[i]
        fourth_val = fourth_list[i]
        
        country_row = [first_val, second_val, third_val, fourth_val]
        
        full_data.append(country_row)
        
    return full_data
    
x_train_set = create_x_data_set(x_train_health_exp, x_train_maternal_dr, x_train_sanitation, x_train_electricity_internet)
x_test_set = create_x_data_set(x_test_health_exp, x_test_maternal_dr, x_test_sanitation, x_test_electricity_internet)

# convert to array for k neighbors classifier
x_train_set = np.array(x_train_set)
x_test_set = np.array(x_test_set)

# get y values (life exp at birth) for training and testing
heading = y_train_df.columns.values    

y_train = y_train_df[heading[0]].tolist()

# for accuracy score calculation
y_test = y_test_df[heading[0]].tolist()

# ~~~~~ using KNeighborsClassifier on feature engineering ~~~~~

classifier_k_5 = KNeighborsClassifier(n_neighbors=5)

# Train the model using the training sets
classifier_k_5.fit(x_train_set, y_train)

# predict using test set
f_eng_y_pred = classifier_k_5.predict(x_test_set)

# check accuracy of KNeighborsClassifier - k = 5
f_eng_accuracy = metrics.accuracy_score(y_test, f_eng_y_pred)
f_eng_accuracy = f_eng_accuracy*100
f_eng_accuracy = "{:.3f}".format(f_eng_accuracy)
f_eng_percent = str(f_eng_accuracy) + "%"

print("Accuracy of feature engineering:", f_eng_percent)

# --------- feature selection: PCA ---------

# reduce dimensionality using PCA from 20 features -> 4

x_train_PCA = x_train_df.values
x_test_PCA = x_test_df.values

pca = PCA(n_components=4)
train_principalComponents = pca.fit_transform(x_train_PCA)
test_princialComponents = pca.fit_transform(x_test_PCA)

principalHeadings = ['principal component 1', 'principal component 2','principal component 3','principal component 4']

# create 4 principal components from original features
x_train_PCA_df = pd.DataFrame(data = train_principalComponents, columns = principalHeadings)
x_test_PCA_df = pd.DataFrame(data = test_princialComponents, columns = principalHeadings)

# combine principal components with y train value
final_train_PCA_df = pd.concat([x_train_PCA_df, y_train_df[['Life expectancy at birth (years)']]], axis = 1)
final_test_PCA_df = pd.concat([x_test_PCA_df, y_test_df[['Life expectancy at birth (years)']]], axis = 1)

# ~~~~~ using KNeighborsClassifier on PCA ~~~~~

# Train the model using the training sets
classifier_k_5.fit(x_train_PCA_df.values, y_train)

# predict using test set
PCA_y_pred = classifier_k_5.predict(x_test_PCA_df.values)

# check accuracy of KNeighborsClassifier - k = 5
PCA_accuracy = metrics.accuracy_score(y_test, PCA_y_pred)
PCA_accuracy = PCA_accuracy*100
PCA_accuracy = "{:.3f}".format(PCA_accuracy)
PCA_percent = str(PCA_accuracy) + "%"

print("Accuracy of PCA:", PCA_percent)

# ------------------ feature selection: first four features ------------------

def get_feature(x_dataframe, position):
    
    feature_list = []
    
    for i in range(len(x_dataframe)):
        
        # get value at index 5 for 1st feature
        feature_list.append(x_dataframe.iloc[i][position])
        
    return feature_list
    
# get first feature - Access to electricity, rural (% of rural population)
x_train_first = get_feature(x_train_df,0)
x_test_first = get_feature(x_test_df,0)

# get second feature - Adjusted savings: particulate emission damage (% of GNI)
x_train_second = get_feature(x_train_df,1)
x_test_second = get_feature(x_test_df,1)

# get third feature - Birth rate, crude (per 1,000 people)
x_train_third = get_feature(x_train_df,2)
x_test_third = get_feature(x_test_df,2)

# get fourth feature - Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total) 
x_train_fourth = get_feature(x_train_df,3)
x_test_fourth = get_feature(x_test_df,3)

# combine all features
x_train_set_firstfour = create_x_data_set(x_train_first, x_train_second, x_train_third, x_train_fourth)
x_test_set_firstfour = create_x_data_set(x_test_first, x_test_second, x_test_third, x_test_fourth)

# turn into array
x_train_firstfour_array = np.array(x_train_set_firstfour)
x_test_firstfour_array = np.array(x_test_set_firstfour)

# ~~~~~ using KNeighborsClassifier on first four features ~~~~~

# Train the model using the training sets
classifier_k_5.fit(x_train_firstfour_array, y_train)

# predict using test set
firstfour_y_pred = classifier_k_5.predict(x_test_firstfour_array)

# check accuracy of KNeighborsClassifier - k = 5
firstfour_accuracy = metrics.accuracy_score(y_test, firstfour_y_pred)
firstfour_accuracy = firstfour_accuracy*100
firstfour_accuracy = "{:.3f}".format(firstfour_accuracy)
firstfour_percent = str(firstfour_accuracy) + "%"

print("Accuracy of first four features:", firstfour_percent)






















